{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-12T12:09:31.781236Z",
     "start_time": "2021-01-12T12:09:30.640206Z"
    }
   },
   "outputs": [],
   "source": [
    "# data preprocess\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import os \n",
    "from itertools import chain, repeat\n",
    "import math\n",
    "import warnings \n",
    "warnings.filterwarnings(action='ignore')\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "# data EDA \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-12T12:09:31.941658Z",
     "start_time": "2021-01-12T12:09:31.783047Z"
    }
   },
   "outputs": [],
   "source": [
    "# file path = data directory\n",
    "file_path = '../태양광예측/'\n",
    "\n",
    "# train, submission data load \n",
    "train = pd.read_csv(file_path+'train/'+'train.csv')\n",
    "submission = pd.read_csv(file_path+'sample_submission.csv')\n",
    "\n",
    "# test load \n",
    "t_test = []\n",
    "\n",
    "for i in range(81):\n",
    "    file_path = '../태양광예측/test/' + str(i) + '.csv'\n",
    "    temp = pd.read_csv(file_path)\n",
    "    t_test.append(temp)\n",
    "    \n",
    "test = pd.concat(t_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T08:03:37.259283Z",
     "start_time": "2021-01-10T08:03:37.241261Z"
    }
   },
   "outputs": [],
   "source": [
    "def base_preprocess(data, is_train=True) : \n",
    "    \n",
    "    if is_train != True :   \n",
    "\n",
    "        pass\n",
    "    \n",
    "    else : \n",
    "        \n",
    "        \n",
    "        # 원래 방법대로 한다면, 마지막 48개행은 target2에 대해 미지의 값을 가진게 아닌가?\n",
    "        # step1 : 하루 뒤, 이틀 뒤 target 값 가져오기 \n",
    "        data['1day_after_target'] = data.shift(-48)['TARGET']\n",
    "        data['2day_after_target'] = data.shift(-96)['TARGET']\n",
    "\n",
    "        # step2 : 1~3일전의 컬럼 및 target 가져오기 \n",
    "        \n",
    "    \n",
    "    #data = data.dropna(axis=0)\n",
    "\n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T08:03:37.355294Z",
     "start_time": "2021-01-10T08:03:37.338283Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape : (52560, 11)\n",
      "df_train shape : (52560, 11)\n",
      "train - df_train shape : 0\n",
      "df_test shape : (27216, 9)\n",
      "test shape : (27216, 9)\n"
     ]
    }
   ],
   "source": [
    "df_train = base_preprocess(train, is_train=True)\n",
    "df_test = base_preprocess(test, is_train=False)\n",
    "\n",
    "print('train shape :' ,train.shape)\n",
    "print('df_train shape :', df_train.shape)\n",
    "print('train - df_train shape :', train.shape[0]-df_train.shape[0])\n",
    "print('df_test shape :', df_test.shape)\n",
    "print('test shape :', test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 천정각"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T08:03:37.719402Z",
     "start_time": "2021-01-10T08:03:37.711414Z"
    }
   },
   "outputs": [],
   "source": [
    "test_day = df_test['Day'].copy() \n",
    "\n",
    "label_list = []\n",
    "\n",
    "for i in range(1,568) : \n",
    "    label_list.append([i]*48)\n",
    "\n",
    "label_list = [item for sublist in label_list for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T08:03:37.890780Z",
     "start_time": "2021-01-10T08:03:37.874563Z"
    }
   },
   "outputs": [],
   "source": [
    "def zenith_angel_def(data, merge_data, is_train=True, label_lists=None) : \n",
    "    \n",
    "    if is_train != True : \n",
    "        \n",
    "        data['Day'] = label_list\n",
    "        merge_data['Day'] = label_list\n",
    "    \n",
    "    # 시간 및 분을 합쳐주는 함수\n",
    "    def time_pil(data) : \n",
    "\n",
    "        texts = str(data['Hour']) + str(data['Minute'])[:1]\n",
    "\n",
    "        return(texts)\n",
    "\n",
    "    data['GHI_less'] = data['DHI'] + data['DNI']\n",
    "    data['Time'] = data[['Hour','Minute']].apply(lambda x: time_pil(x), axis=1)\n",
    "    data['GHI_less'] = data['GHI_less'].astype('int') \n",
    "\n",
    "    f = data[data['GHI_less']>0].groupby(['Day']).head(1)[['Day','Time','T','GHI_less']]\n",
    "    f.columns = ['Day','First_time','First_T','First_GHI_less']\n",
    "\n",
    "    l = data[data['GHI_less']>0].groupby(['Day']).tail(1)[['Day','Time','T','GHI_less']]\n",
    "    l.columns = ['Day','Last_time','Last_T','Last_GHI_less']\n",
    "    \n",
    "    f_l = pd.merge(f, l, how='inner', on=['Day']) \n",
    "    \n",
    "    def sun_hour(data) : \n",
    "\n",
    "        hours = (int(str(data['Last_time'])[:2]) - int(str(data['First_time'])[0]))*60\n",
    "\n",
    "        return(hours)\n",
    "\n",
    "    def sun_minute(data) : \n",
    "\n",
    "        minutes = (int(str(data['Last_time'][2])) - int(str(data['First_time'][1])))*10\n",
    "\n",
    "        return(minutes)\n",
    "\n",
    "    f_l['SUN_hour'] = f_l[['First_time','Last_time']].apply(lambda x:sun_hour(x), axis=1)\n",
    "    f_l['SUN_minute'] = f_l[['First_time','Last_time']].apply(lambda x:sun_minute(x), axis=1)\n",
    "    \n",
    "    f_l['SUN_time'] = f_l['SUN_hour'] + f_l['SUN_minute']\n",
    "    \n",
    "    temp = pd.merge(data, f_l, how='inner', on='Day')\n",
    "    temp['Time'] = temp[['Hour','Minute']].apply(lambda x: time_pil(x), axis=1)\n",
    "    temp[['Time','First_time','Last_time']] = temp[['Time','First_time','Last_time']].astype('int')\n",
    "    \n",
    "    temp2 = temp[(temp['Time']>=temp['First_time']) & (temp['Time']<=temp['Last_time'])]  \n",
    "    \n",
    "    zenith_angle_lists = []\n",
    "\n",
    "    for day in temp2.Day.unique() : \n",
    "\n",
    "        temp3 = temp2[temp2['Day']==day]\n",
    "        zenith_angles = np.arange(0,181,180/(temp3['SUN_time'].iloc[0]/30))\n",
    "\n",
    "        for num in range(len(zenith_angles)) : \n",
    "\n",
    "            if zenith_angles[num] < 90 : \n",
    "                zenith_angles[num] = 90 - zenith_angles[num]\n",
    "                \n",
    "            elif zenith_angles[num] == 0 :\n",
    "                zenith_angles[num] = 1\n",
    "\n",
    "            else : \n",
    "                zenith_angles[num] = zenith_angles[num] - 90\n",
    "\n",
    "        zenith_angle_lists.append(zenith_angles)\n",
    "        \n",
    "    zenith_angle_lists = [item for sublist in zenith_angle_lists for item in sublist]\n",
    "    temp2['zenith_angle'] = zenith_angle_lists\n",
    "    \n",
    "    final = pd.merge(merge_data, temp2[['Day','Hour','Minute','zenith_angle']],\n",
    "                    how='left', on=['Day','Hour','Minute'])\n",
    "    \n",
    "    final = final.fillna(90) \n",
    "    \n",
    "    final = pd.merge(final, temp2[['Day','SUN_time']].drop_duplicates(),\n",
    "                    how='inner', on=['Day'])\n",
    "    \n",
    "    return(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T08:03:41.811975Z",
     "start_time": "2021-01-10T08:03:38.078982Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train = zenith_angel_def(data = df_train, merge_data = df_train\n",
    "                            , is_train=True, label_lists=None)\n",
    "\n",
    "df_test = zenith_angel_def(data = df_test, merge_data = df_test\n",
    "                            , is_train=False, label_lists=label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T08:03:43.780442Z",
     "start_time": "2021-01-10T08:03:41.814978Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train shape : (52560, 16)\n",
      "df_test shape : (27216, 14)\n"
     ]
    }
   ],
   "source": [
    "def custom_ghi(data) : \n",
    "    \n",
    "    answer = data['DHI'] + (data['DNI']*math.cos(data['zenith_angle']))\n",
    "    \n",
    "    return answer \n",
    "\n",
    "df_train['GHI'] = df_train[['DHI','DNI','zenith_angle']].apply(lambda x: custom_ghi(x), axis=1)\n",
    "df_test['GHI'] = df_test[['DHI','DNI','zenith_angle']].apply(lambda x: custom_ghi(x), axis=1)\n",
    "\n",
    "print('df_train shape :', df_train.shape)\n",
    "print('df_test shape :', df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T08:03:43.795981Z",
     "start_time": "2021-01-10T08:03:43.783430Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test['Day'] = test_day.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6일간의 데이터 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T08:03:43.812001Z",
     "start_time": "2021-01-10T08:03:43.797986Z"
    }
   },
   "outputs": [],
   "source": [
    "label_list = []\n",
    "\n",
    "for i in range(81) : \n",
    "    label_list.append([i]*336)\n",
    "\n",
    "label_list = [item for sublist in label_list for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T08:03:43.826997Z",
     "start_time": "2021-01-10T08:03:43.813988Z"
    }
   },
   "outputs": [],
   "source": [
    "def day_of_six(dataset) : \n",
    "    \n",
    "    data = dataset.copy()\n",
    "    \n",
    "    colnames = []\n",
    "    temp = data.copy()\n",
    "    \n",
    "    \n",
    "    # 6일전까지의 데이터 구축 \n",
    "    for six_days in range(1,7,1) : \n",
    "        \n",
    "        for colname in data.columns : \n",
    "            \n",
    "            if colname in ('Day','Hour','Minute','1day_after_target', '2day_after_target',\n",
    "                          'GHI_less','Time') :\n",
    "                \n",
    "                pass \n",
    "            \n",
    "            else : \n",
    "                \n",
    "                temp[f'{six_days}_preday_{colname}'] = temp.shift(six_days*48)[colname]\n",
    "                \n",
    "                if six_days == 1 :\n",
    "                \n",
    "                    colnames.append(colname)\n",
    "    \n",
    "    # 6일전까지의 데이터를 바탕으로 새로운 변수 구축 \n",
    "    day_0_2 = ['6_preday_','5_preday_','4_preday_']\n",
    "    day_2_4 = ['4_preday_','3_preday_','2_preday_']\n",
    "    day_4_6 = ['2_preday_','1_preday_','']\n",
    "    \n",
    "    def dev_cv(data) : \n",
    "        \n",
    "        means = np.mean(data)\n",
    "        sds = np.std(data)\n",
    "        \n",
    "        if means == 0 : \n",
    "            cv = 0 \n",
    "        else : \n",
    "            cv = sds/means\n",
    "            \n",
    "        return(cv)\n",
    "    \n",
    "    \n",
    "    for colname in colnames : \n",
    "        \n",
    "        # 1) 구간별 평균 (0~2일차, 2~4일차, 4~6일차)\n",
    "        data[f'day_0to2_{colname}_mean'] = temp[[i+colname for i in day_0_2]].apply('mean',axis=1)\n",
    "        data[f'day_2to4_{colname}_mean'] = temp[[i+colname for i in day_2_4]].apply('mean',axis=1)\n",
    "        data[f'day_4to6_{colname}_mean'] = temp[[i+colname for i in day_4_6]].apply('mean',axis=1)\n",
    "        \n",
    "        # 2) 구간별 변동계수 (0~2일차, 2~4일차, 4~6일차)\n",
    "        data[f'day_0to2_{colname}_cv'] = temp[[i+colname for i in day_0_2]].apply(lambda x: dev_cv(x),axis=1)\n",
    "        data[f'day_2to4_{colname}_cv'] = temp[[i+colname for i in day_2_4]].apply(lambda x: dev_cv(x),axis=1)\n",
    "        data[f'day_4to6_{colname}_cv'] = temp[[i+colname for i in day_4_6]].apply(lambda x: dev_cv(x),axis=1)\n",
    "        \n",
    "        \n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T08:05:16.018709Z",
     "start_time": "2021-01-10T08:03:43.828997Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train = day_of_six(dataset=df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T08:05:16.033890Z",
     "start_time": "2021-01-10T08:05:16.020716Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test['label'] = label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T08:06:13.916374Z",
     "start_time": "2021-01-10T08:05:16.035893Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0st dataset complete!!\n",
      "1st dataset complete!!\n",
      "2st dataset complete!!\n",
      "3st dataset complete!!\n",
      "4st dataset complete!!\n",
      "5st dataset complete!!\n",
      "6st dataset complete!!\n",
      "7st dataset complete!!\n",
      "8st dataset complete!!\n",
      "9st dataset complete!!\n",
      "10st dataset complete!!\n",
      "11st dataset complete!!\n",
      "12st dataset complete!!\n",
      "13st dataset complete!!\n",
      "14st dataset complete!!\n",
      "15st dataset complete!!\n",
      "16st dataset complete!!\n",
      "17st dataset complete!!\n",
      "18st dataset complete!!\n",
      "19st dataset complete!!\n",
      "20st dataset complete!!\n",
      "21st dataset complete!!\n",
      "22st dataset complete!!\n",
      "23st dataset complete!!\n",
      "24st dataset complete!!\n",
      "25st dataset complete!!\n",
      "26st dataset complete!!\n",
      "27st dataset complete!!\n",
      "28st dataset complete!!\n",
      "29st dataset complete!!\n",
      "30st dataset complete!!\n",
      "31st dataset complete!!\n",
      "32st dataset complete!!\n",
      "33st dataset complete!!\n",
      "34st dataset complete!!\n",
      "35st dataset complete!!\n",
      "36st dataset complete!!\n",
      "37st dataset complete!!\n",
      "38st dataset complete!!\n",
      "39st dataset complete!!\n",
      "40st dataset complete!!\n",
      "41st dataset complete!!\n",
      "42st dataset complete!!\n",
      "43st dataset complete!!\n",
      "44st dataset complete!!\n",
      "45st dataset complete!!\n",
      "46st dataset complete!!\n",
      "47st dataset complete!!\n",
      "48st dataset complete!!\n",
      "49st dataset complete!!\n",
      "50st dataset complete!!\n",
      "51st dataset complete!!\n",
      "52st dataset complete!!\n",
      "53st dataset complete!!\n",
      "54st dataset complete!!\n",
      "55st dataset complete!!\n",
      "56st dataset complete!!\n",
      "57st dataset complete!!\n",
      "58st dataset complete!!\n",
      "59st dataset complete!!\n",
      "60st dataset complete!!\n",
      "61st dataset complete!!\n",
      "62st dataset complete!!\n",
      "63st dataset complete!!\n",
      "64st dataset complete!!\n",
      "65st dataset complete!!\n",
      "66st dataset complete!!\n",
      "67st dataset complete!!\n",
      "68st dataset complete!!\n",
      "69st dataset complete!!\n",
      "70st dataset complete!!\n",
      "71st dataset complete!!\n",
      "72st dataset complete!!\n",
      "73st dataset complete!!\n",
      "74st dataset complete!!\n",
      "75st dataset complete!!\n",
      "76st dataset complete!!\n",
      "77st dataset complete!!\n",
      "78st dataset complete!!\n",
      "79st dataset complete!!\n",
      "80st dataset complete!!\n"
     ]
    }
   ],
   "source": [
    "df_testset = []\n",
    "\n",
    "for i in range(81) : \n",
    "    \n",
    "    test_data = df_test[df_test['label']==i].drop(['label'],axis=1)\n",
    "    temp_test = day_of_six(dataset=test_data)\n",
    "    \n",
    "    df_testset.append(temp_test)\n",
    "    print(f'{i}st dataset complete!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T08:06:48.687387Z",
     "start_time": "2021-01-10T08:06:48.636840Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test = pd.concat(df_testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N일간의 평균 데이터 (시간 및 분 기준의 N일전)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T06:44:10.761951Z",
     "start_time": "2021-01-10T06:44:10.747434Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_2day_variable(df_train, rolling_num) : \n",
    "    \n",
    "    hour_lists = []\n",
    "\n",
    "    numbers = list(np.arange(0,24,1))\n",
    "    n_list = [2]*24\n",
    "\n",
    "    hours_range = list(chain.from_iterable((repeat(number, n) for (number, n) in zip(numbers, n_list))))\n",
    "\n",
    "    for (hours,minutes) in zip(hours_range,list(np.arange(0,31,30))*30) : \n",
    "\n",
    "        hour_lists.append(df_train[(df_train['Hour']==hours) & (df_train['Minute']==minutes)])\n",
    "\n",
    "    for num in range(len(hour_lists)) : \n",
    "\n",
    "        hour_lists[num][f'{rolling_num}days_mean_DHI'] = hour_lists[num]['DHI'].rolling(rolling_num).mean()\n",
    "        hour_lists[num][f'{rolling_num}days_mean_DNI'] = hour_lists[num]['DNI'].rolling(rolling_num).mean()\n",
    "        #hour_lists[num][f'{rolling_num}days_mean_T'] = hour_lists[num]['T'].rolling(rolling_num).mean()\n",
    "        #hour_lists[num][f'{rolling_num}days_mean_RH'] = hour_lists[num]['RH'].rolling(rolling_num).mean()\n",
    "        hour_lists[num][f'{rolling_num}days_mean_WS'] = hour_lists[num]['WS'].rolling(rolling_num).mean() \n",
    "        hour_lists[num][f'{rolling_num}days_mean_TARGET'] = hour_lists[num]['TARGET'].rolling(rolling_num).mean()\n",
    "        hour_lists[num][f'{rolling_num}days_mean_SUN_time'] = hour_lists[num]['SUN_time'].rolling(rolling_num).mean()\n",
    "        hour_lists[num][f'{rolling_num}days_mean_zenith_angle'] = hour_lists[num]['zenith_angle'].rolling(rolling_num).mean()\n",
    "        hour_lists[num][f'{rolling_num}days_mean_GHI'] = hour_lists[num]['GHI'].rolling(rolling_num).mean()\n",
    "        \n",
    "    df_train = pd.concat(hour_lists).sort_index() \n",
    "    \n",
    "    return(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T06:44:11.074292Z",
     "start_time": "2021-01-10T06:44:10.764029Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train = train_2day_variable(df_train, rolling_num=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T06:44:11.311613Z",
     "start_time": "2021-01-10T06:44:11.287070Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_2day_variable(df_test, rolling_num) : \n",
    "\n",
    "label_list = []\n",
    "\n",
    "for i in range(81) : \n",
    "    label_list.append([i]*336)\n",
    "\n",
    "label_list = [item for sublist in label_list for item in sublist]\n",
    "\n",
    "df_test['label'] = label_list \n",
    "\n",
    "test_dataset = []\n",
    "\n",
    "for label_num in df_test.label.unique() : \n",
    "\n",
    "    hour_lists = []\n",
    "    numbers = list(np.arange(0,24,1))\n",
    "    n_list = [2]*24\n",
    "    df_test2 = df_test[df_test['label']==label_num]\n",
    "\n",
    "    hours_range = list(chain.from_iterable((repeat(number, n) for (number, n) in zip(numbers, n_list))))\n",
    "\n",
    "    for (hours,minutes) in zip(hours_range,list(np.arange(0,31,30))*30) : \n",
    "\n",
    "        hour_lists.append(df_test2[(df_test2['Hour']==hours) & (df_test2['Minute']==minutes)])\n",
    "\n",
    "    for num in range(len(hour_lists)) : \n",
    "\n",
    "        hour_lists[num][f'{rolling_num}days_mean_DHI'] = hour_lists[num]['DHI'].rolling(rolling_num).mean()\n",
    "        hour_lists[num][f'{rolling_num}days_mean_DNI'] = hour_lists[num]['DNI'].rolling(rolling_num).mean()\n",
    "        #hour_lists[num][f'{rolling_num}days_mean_T'] = hour_lists[num]['T'].rolling(rolling_num).mean()\n",
    "        #hour_lists[num][f'{rolling_num}days_mean_RH'] = hour_lists[num]['RH'].rolling(rolling_num).mean()\n",
    "        hour_lists[num][f'{rolling_num}days_mean_WS'] = hour_lists[num]['WS'].rolling(rolling_num).mean()\n",
    "        hour_lists[num][f'{rolling_num}days_mean_TARGET'] = hour_lists[num]['TARGET'].rolling(rolling_num).mean()\n",
    "        hour_lists[num][f'{rolling_num}days_mean_SUN_time'] = hour_lists[num]['SUN_time'].rolling(rolling_num).mean()\n",
    "        hour_lists[num][f'{rolling_num}days_mean_zenith_angle'] = hour_lists[num]['zenith_angle'].rolling(rolling_num).mean()\n",
    "        hour_lists[num][f'{rolling_num}days_mean_GHI'] = hour_lists[num]['GHI'].rolling(rolling_num).mean()\n",
    "\n",
    "    temp = pd.concat(hour_lists)\n",
    "    test_dataset.append(temp)\n",
    "\n",
    "df_test = pd.concat(test_dataset).reset_index().sort_values(['label','index'])\n",
    "df_test = df_test.drop(['index'],axis=1).reset_index(drop=True) \n",
    "\n",
    "return(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T06:44:33.620343Z",
     "start_time": "2021-01-10T06:44:14.060964Z"
    }
   },
   "outputs": [],
   "source": [
    "[df_test = test_2day_variable(df_test, rolling_num=3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T06:44:33.635896Z",
     "start_time": "2021-01-10T06:44:33.622347Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train shape : (52560, 23)\n",
      "df_test shape : (27216, 22)\n"
     ]
    }
   ],
   "source": [
    "print('df_train shape :', df_train.shape)\n",
    "print('df_test shape :', df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[df_train['RH']!=100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T08:07:00.094336Z",
     "start_time": "2021-01-10T08:07:00.084324Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train shape : (52560, 70)\n",
      "df_test shape : (27216, 68)\n"
     ]
    }
   ],
   "source": [
    "print('df_train shape :', df_train.shape)\n",
    "print('df_test shape :', df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T08:07:00.793186Z",
     "start_time": "2021-01-10T08:07:00.706161Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train shape : (52368, 66)\n",
      "df_test shape : (3888, 64)\n"
     ]
    }
   ],
   "source": [
    "df_train = df_train.dropna()\n",
    "df_train = df_train.drop(['Day','Minute','GHI_less','Time'], axis=1)\n",
    "df_train[['Hour']] = df_train[['Hour']].astype('category')\n",
    "\n",
    "df_test = df_test.dropna()\n",
    "df_test = df_test[df_test['Day']==6]\n",
    "df_test = df_test.drop(['Day','Minute','GHI_less','Time'], axis=1)\n",
    "df_test[['Hour']] = df_test[['Hour']].astype('category')\n",
    "\n",
    "print('df_train shape :', df_train.shape)\n",
    "print('df_test shape :', df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T08:07:02.396451Z",
     "start_time": "2021-01-10T08:07:02.317472Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "drop_columns = ['1day_after_target','2day_after_target']\n",
    "y1 = '1day_after_target'\n",
    "y2 = '2day_after_target'\n",
    "\n",
    "X_train_1, X_valid_1, Y_train_1, Y_valid_1 = train_test_split(df_train.drop(drop_columns,axis=1), df_train[y1], test_size=0.33, random_state=0)\n",
    "X_train_2, X_valid_2, Y_train_2, Y_valid_2 = train_test_split(df_train.drop(drop_columns,axis=1), df_train[y2], test_size=0.33, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T08:07:02.569945Z",
     "start_time": "2021-01-10T08:07:02.560944Z"
    }
   },
   "outputs": [],
   "source": [
    "quantiles = [0.06, 0.17, 0.25, 0.36, 0.52, 0.63, 0.74, 0.85, 0.95]\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# Get the model and the predictions in (a) - (b)\n",
    "def LGBM(q, X_train, Y_train, X_valid, Y_valid, df_test):\n",
    "    \n",
    "    # (a) Modeling  \n",
    "    model = LGBMRegressor(objective='quantile', alpha=q, \n",
    "                         n_estimators=10000, bagging_fraction=0.7, learning_rate=0.027, subsample=0.7)                   \n",
    "                         \n",
    "                         \n",
    "    model.fit(X_train, Y_train, eval_metric = ['quantile'], \n",
    "          eval_set=[(X_valid, Y_valid)], early_stopping_rounds=300, verbose=500)\n",
    "    \n",
    "\n",
    "    # (b) Predictions\n",
    "    pred = pd.Series(model.predict(df_test).round(2))\n",
    "    return pred, model\n",
    "\n",
    "# Target 예측\n",
    "\n",
    "def train_data(X_train, Y_train, X_valid, Y_valid, df_test):\n",
    "\n",
    "    LGBM_models=[]\n",
    "    LGBM_actual_pred = pd.DataFrame()\n",
    "\n",
    "    for q in quantiles:\n",
    "        print(q)\n",
    "        pred , model = LGBM(q, X_train, Y_train, X_valid, Y_valid, df_test)\n",
    "        LGBM_models.append(model)\n",
    "        LGBM_actual_pred = pd.concat([LGBM_actual_pred,pred],axis=1)\n",
    "\n",
    "    LGBM_actual_pred.columns=quantiles\n",
    "    \n",
    "    return LGBM_models, LGBM_actual_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T08:13:58.171721Z",
     "start_time": "2021-01-10T08:07:03.250207Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=0.7 will be ignored. Current value: bagging_fraction=0.7\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 0.880712\n",
      "[1000]\tvalid_0's quantile: 0.883013\n",
      "Early stopping, best iteration is:\n",
      "[715]\tvalid_0's quantile: 0.879751\n",
      "0.17\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=0.7 will be ignored. Current value: bagging_fraction=0.7\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 1.86602\n",
      "[1000]\tvalid_0's quantile: 1.81859\n",
      "[1500]\tvalid_0's quantile: 1.80321\n",
      "[2000]\tvalid_0's quantile: 1.79478\n",
      "[2500]\tvalid_0's quantile: 1.7878\n",
      "[3000]\tvalid_0's quantile: 1.78418\n",
      "[3500]\tvalid_0's quantile: 1.78121\n",
      "[4000]\tvalid_0's quantile: 1.77781\n",
      "[4500]\tvalid_0's quantile: 1.77624\n",
      "[5000]\tvalid_0's quantile: 1.77266\n",
      "[5500]\tvalid_0's quantile: 1.77034\n",
      "[6000]\tvalid_0's quantile: 1.76918\n",
      "Early stopping, best iteration is:\n",
      "[5875]\tvalid_0's quantile: 1.76748\n",
      "0.25\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=0.7 will be ignored. Current value: bagging_fraction=0.7\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 2.26274\n",
      "[1000]\tvalid_0's quantile: 2.18737\n",
      "[1500]\tvalid_0's quantile: 2.15104\n",
      "[2000]\tvalid_0's quantile: 2.12631\n",
      "[2500]\tvalid_0's quantile: 2.1062\n",
      "[3000]\tvalid_0's quantile: 2.09614\n",
      "[3500]\tvalid_0's quantile: 2.08701\n",
      "[4000]\tvalid_0's quantile: 2.08245\n",
      "[4500]\tvalid_0's quantile: 2.07572\n",
      "[5000]\tvalid_0's quantile: 2.07098\n",
      "[5500]\tvalid_0's quantile: 2.06812\n",
      "[6000]\tvalid_0's quantile: 2.06349\n",
      "[6500]\tvalid_0's quantile: 2.06504\n",
      "Early stopping, best iteration is:\n",
      "[6223]\tvalid_0's quantile: 2.06187\n",
      "0.36\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=0.7 will be ignored. Current value: bagging_fraction=0.7\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 2.47526\n",
      "[1000]\tvalid_0's quantile: 2.38006\n",
      "[1500]\tvalid_0's quantile: 2.34089\n",
      "[2000]\tvalid_0's quantile: 2.31019\n",
      "[2500]\tvalid_0's quantile: 2.29819\n",
      "[3000]\tvalid_0's quantile: 2.2935\n",
      "[3500]\tvalid_0's quantile: 2.28662\n",
      "[4000]\tvalid_0's quantile: 2.27492\n",
      "[4500]\tvalid_0's quantile: 2.26611\n",
      "[5000]\tvalid_0's quantile: 2.25909\n",
      "[5500]\tvalid_0's quantile: 2.24954\n",
      "[6000]\tvalid_0's quantile: 2.24365\n",
      "[6500]\tvalid_0's quantile: 2.23933\n",
      "[7000]\tvalid_0's quantile: 2.23551\n",
      "[7500]\tvalid_0's quantile: 2.23121\n",
      "[8000]\tvalid_0's quantile: 2.22806\n",
      "[8500]\tvalid_0's quantile: 2.22553\n",
      "[9000]\tvalid_0's quantile: 2.22086\n",
      "[9500]\tvalid_0's quantile: 2.21748\n",
      "[10000]\tvalid_0's quantile: 2.21611\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[9999]\tvalid_0's quantile: 2.21611\n",
      "0.52\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=0.7 will be ignored. Current value: bagging_fraction=0.7\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 2.41657\n",
      "[1000]\tvalid_0's quantile: 2.34101\n",
      "[1500]\tvalid_0's quantile: 2.315\n",
      "[2000]\tvalid_0's quantile: 2.3022\n",
      "[2500]\tvalid_0's quantile: 2.28301\n",
      "[3000]\tvalid_0's quantile: 2.25928\n",
      "[3500]\tvalid_0's quantile: 2.25641\n",
      "[4000]\tvalid_0's quantile: 2.24674\n",
      "[4500]\tvalid_0's quantile: 2.23884\n",
      "[5000]\tvalid_0's quantile: 2.2283\n",
      "[5500]\tvalid_0's quantile: 2.2257\n",
      "[6000]\tvalid_0's quantile: 2.22075\n",
      "[6500]\tvalid_0's quantile: 2.21607\n",
      "[7000]\tvalid_0's quantile: 2.21403\n",
      "[7500]\tvalid_0's quantile: 2.20558\n",
      "[8000]\tvalid_0's quantile: 2.20463\n",
      "[8500]\tvalid_0's quantile: 2.2028\n",
      "[9000]\tvalid_0's quantile: 2.20069\n",
      "[9500]\tvalid_0's quantile: 2.19849\n",
      "[10000]\tvalid_0's quantile: 2.19682\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[9998]\tvalid_0's quantile: 2.19682\n",
      "0.63\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=0.7 will be ignored. Current value: bagging_fraction=0.7\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 2.12581\n",
      "[1000]\tvalid_0's quantile: 2.07479\n",
      "[1500]\tvalid_0's quantile: 2.05262\n",
      "[2000]\tvalid_0's quantile: 2.03357\n",
      "[2500]\tvalid_0's quantile: 2.01984\n",
      "[3000]\tvalid_0's quantile: 2.00842\n",
      "[3500]\tvalid_0's quantile: 2.00139\n",
      "[4000]\tvalid_0's quantile: 1.99763\n",
      "[4500]\tvalid_0's quantile: 1.99371\n",
      "[5000]\tvalid_0's quantile: 1.99166\n",
      "[5500]\tvalid_0's quantile: 1.98814\n",
      "[6000]\tvalid_0's quantile: 1.97999\n",
      "[6500]\tvalid_0's quantile: 1.97916\n",
      "Early stopping, best iteration is:\n",
      "[6347]\tvalid_0's quantile: 1.97799\n",
      "0.74\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=0.7 will be ignored. Current value: bagging_fraction=0.7\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 1.67046\n",
      "[1000]\tvalid_0's quantile: 1.65044\n",
      "[1500]\tvalid_0's quantile: 1.63552\n",
      "[2000]\tvalid_0's quantile: 1.6261\n",
      "[2500]\tvalid_0's quantile: 1.6235\n",
      "[3000]\tvalid_0's quantile: 1.61938\n",
      "[3500]\tvalid_0's quantile: 1.6166\n",
      "[4000]\tvalid_0's quantile: 1.61384\n",
      "[4500]\tvalid_0's quantile: 1.61202\n",
      "[5000]\tvalid_0's quantile: 1.60955\n",
      "[5500]\tvalid_0's quantile: 1.60806\n",
      "[6000]\tvalid_0's quantile: 1.60688\n",
      "[6500]\tvalid_0's quantile: 1.60635\n",
      "[7000]\tvalid_0's quantile: 1.60662\n",
      "Early stopping, best iteration is:\n",
      "[6831]\tvalid_0's quantile: 1.60624\n",
      "0.85\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=0.7 will be ignored. Current value: bagging_fraction=0.7\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 1.07428\n",
      "[1000]\tvalid_0's quantile: 1.06415\n",
      "[1500]\tvalid_0's quantile: 1.0615\n",
      "[2000]\tvalid_0's quantile: 1.06074\n",
      "[2500]\tvalid_0's quantile: 1.06024\n",
      "Early stopping, best iteration is:\n",
      "[2690]\tvalid_0's quantile: 1.06011\n",
      "0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=0.7 will be ignored. Current value: bagging_fraction=0.7\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 0.451179\n",
      "[1000]\tvalid_0's quantile: 0.450983\n",
      "Early stopping, best iteration is:\n",
      "[878]\tvalid_0's quantile: 0.450585\n",
      "0.06\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=0.7 will be ignored. Current value: bagging_fraction=0.7\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 0.900653\n",
      "[1000]\tvalid_0's quantile: 0.895337\n",
      "[1500]\tvalid_0's quantile: 0.894567\n",
      "Early stopping, best iteration is:\n",
      "[1588]\tvalid_0's quantile: 0.894167\n",
      "0.17\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=0.7 will be ignored. Current value: bagging_fraction=0.7\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 1.91498\n",
      "[1000]\tvalid_0's quantile: 1.8551\n",
      "[1500]\tvalid_0's quantile: 1.82991\n",
      "[2000]\tvalid_0's quantile: 1.81354\n",
      "[2500]\tvalid_0's quantile: 1.80685\n",
      "[3000]\tvalid_0's quantile: 1.80181\n",
      "[3500]\tvalid_0's quantile: 1.79844\n",
      "[4000]\tvalid_0's quantile: 1.79647\n",
      "[4500]\tvalid_0's quantile: 1.79532\n",
      "[5000]\tvalid_0's quantile: 1.79361\n",
      "[5500]\tvalid_0's quantile: 1.79293\n",
      "[6000]\tvalid_0's quantile: 1.79122\n",
      "[6500]\tvalid_0's quantile: 1.78837\n",
      "[7000]\tvalid_0's quantile: 1.78751\n",
      "Early stopping, best iteration is:\n",
      "[7194]\tvalid_0's quantile: 1.78659\n",
      "0.25\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=0.7 will be ignored. Current value: bagging_fraction=0.7\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 2.29775\n",
      "[1000]\tvalid_0's quantile: 2.22423\n",
      "[1500]\tvalid_0's quantile: 2.18115\n",
      "[2000]\tvalid_0's quantile: 2.15442\n",
      "[2500]\tvalid_0's quantile: 2.13529\n",
      "[3000]\tvalid_0's quantile: 2.10983\n",
      "[3500]\tvalid_0's quantile: 2.09523\n",
      "[4000]\tvalid_0's quantile: 2.08271\n",
      "[4500]\tvalid_0's quantile: 2.07374\n",
      "[5000]\tvalid_0's quantile: 2.06579\n",
      "[5500]\tvalid_0's quantile: 2.05908\n",
      "[6000]\tvalid_0's quantile: 2.05593\n",
      "[6500]\tvalid_0's quantile: 2.05238\n",
      "[7000]\tvalid_0's quantile: 2.04895\n",
      "[7500]\tvalid_0's quantile: 2.04779\n",
      "Early stopping, best iteration is:\n",
      "[7525]\tvalid_0's quantile: 2.04767\n",
      "0.36\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=0.7 will be ignored. Current value: bagging_fraction=0.7\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 2.51781\n",
      "[1000]\tvalid_0's quantile: 2.41554\n",
      "[1500]\tvalid_0's quantile: 2.37139\n",
      "[2000]\tvalid_0's quantile: 2.31108\n",
      "[2500]\tvalid_0's quantile: 2.28384\n",
      "[3000]\tvalid_0's quantile: 2.2643\n",
      "[3500]\tvalid_0's quantile: 2.2584\n",
      "[4000]\tvalid_0's quantile: 2.25208\n",
      "[4500]\tvalid_0's quantile: 2.24672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5000]\tvalid_0's quantile: 2.24161\n",
      "[5500]\tvalid_0's quantile: 2.23547\n",
      "[6000]\tvalid_0's quantile: 2.22891\n",
      "[6500]\tvalid_0's quantile: 2.22519\n",
      "[7000]\tvalid_0's quantile: 2.22304\n",
      "[7500]\tvalid_0's quantile: 2.2207\n",
      "[8000]\tvalid_0's quantile: 2.2172\n",
      "[8500]\tvalid_0's quantile: 2.21545\n",
      "[9000]\tvalid_0's quantile: 2.21305\n",
      "[9500]\tvalid_0's quantile: 2.21162\n",
      "[10000]\tvalid_0's quantile: 2.20714\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[9997]\tvalid_0's quantile: 2.20714\n",
      "0.52\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=0.7 will be ignored. Current value: bagging_fraction=0.7\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 2.4168\n",
      "[1000]\tvalid_0's quantile: 2.32928\n",
      "[1500]\tvalid_0's quantile: 2.29755\n",
      "[2000]\tvalid_0's quantile: 2.27131\n",
      "[2500]\tvalid_0's quantile: 2.24741\n",
      "[3000]\tvalid_0's quantile: 2.2252\n",
      "[3500]\tvalid_0's quantile: 2.21327\n",
      "[4000]\tvalid_0's quantile: 2.20335\n",
      "[4500]\tvalid_0's quantile: 2.19492\n",
      "[5000]\tvalid_0's quantile: 2.18927\n",
      "[5500]\tvalid_0's quantile: 2.18313\n",
      "[6000]\tvalid_0's quantile: 2.18032\n",
      "[6500]\tvalid_0's quantile: 2.17824\n",
      "[7000]\tvalid_0's quantile: 2.17619\n",
      "[7500]\tvalid_0's quantile: 2.16758\n",
      "[8000]\tvalid_0's quantile: 2.16309\n",
      "[8500]\tvalid_0's quantile: 2.16225\n",
      "[9000]\tvalid_0's quantile: 2.16098\n",
      "[9500]\tvalid_0's quantile: 2.16014\n",
      "[10000]\tvalid_0's quantile: 2.15916\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\tvalid_0's quantile: 2.15916\n",
      "0.63\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=0.7 will be ignored. Current value: bagging_fraction=0.7\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 2.15878\n",
      "[1000]\tvalid_0's quantile: 2.10249\n",
      "[1500]\tvalid_0's quantile: 2.07805\n",
      "[2000]\tvalid_0's quantile: 2.05609\n",
      "[2500]\tvalid_0's quantile: 2.03863\n",
      "[3000]\tvalid_0's quantile: 2.02884\n",
      "[3500]\tvalid_0's quantile: 2.02078\n",
      "[4000]\tvalid_0's quantile: 2.01342\n",
      "[4500]\tvalid_0's quantile: 2.01107\n",
      "[5000]\tvalid_0's quantile: 2.00584\n",
      "[5500]\tvalid_0's quantile: 2.00305\n",
      "[6000]\tvalid_0's quantile: 1.99939\n",
      "[6500]\tvalid_0's quantile: 1.99356\n",
      "[7000]\tvalid_0's quantile: 1.98924\n",
      "[7500]\tvalid_0's quantile: 1.98755\n",
      "Early stopping, best iteration is:\n",
      "[7485]\tvalid_0's quantile: 1.98721\n",
      "0.74\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=0.7 will be ignored. Current value: bagging_fraction=0.7\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 1.71507\n",
      "[1000]\tvalid_0's quantile: 1.68938\n",
      "[1500]\tvalid_0's quantile: 1.67944\n",
      "[2000]\tvalid_0's quantile: 1.66849\n",
      "[2500]\tvalid_0's quantile: 1.66014\n",
      "[3000]\tvalid_0's quantile: 1.65616\n",
      "[3500]\tvalid_0's quantile: 1.65396\n",
      "[4000]\tvalid_0's quantile: 1.64968\n",
      "[4500]\tvalid_0's quantile: 1.64583\n",
      "[5000]\tvalid_0's quantile: 1.64364\n",
      "[5500]\tvalid_0's quantile: 1.64247\n",
      "[6000]\tvalid_0's quantile: 1.64397\n",
      "Early stopping, best iteration is:\n",
      "[5714]\tvalid_0's quantile: 1.64144\n",
      "0.85\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=0.7 will be ignored. Current value: bagging_fraction=0.7\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 1.13186\n",
      "[1000]\tvalid_0's quantile: 1.1191\n",
      "[1500]\tvalid_0's quantile: 1.11598\n",
      "[2000]\tvalid_0's quantile: 1.11501\n",
      "[2500]\tvalid_0's quantile: 1.11432\n",
      "[3000]\tvalid_0's quantile: 1.11406\n",
      "Early stopping, best iteration is:\n",
      "[2918]\tvalid_0's quantile: 1.11401\n",
      "0.95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=0.7 will be ignored. Current value: bagging_fraction=0.7\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 0.454925\n",
      "[1000]\tvalid_0's quantile: 0.450205\n",
      "Early stopping, best iteration is:\n",
      "[742]\tvalid_0's quantile: 0.449456\n"
     ]
    }
   ],
   "source": [
    "# Target1\n",
    "models_1, results_1 = train_data(X_train_1, Y_train_1, X_valid_1, Y_valid_1, df_test)\n",
    "\n",
    "# Target2\n",
    "models_2, results_2 = train_data(X_train_2, Y_train_2, X_valid_2, Y_valid_2, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T08:13:58.213470Z",
     "start_time": "2021-01-10T08:13:58.171721Z"
    }
   },
   "outputs": [],
   "source": [
    "submission.loc[submission.id.str.contains(\"Day7\"), \"q_0.1\":] = results_1.sort_index().values\n",
    "submission.loc[submission.id.str.contains(\"Day8\"), \"q_0.1\":] = results_2.sort_index().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T08:13:58.309499Z",
     "start_time": "2021-01-10T08:13:58.213470Z"
    }
   },
   "outputs": [],
   "source": [
    "submission.to_csv('../태양광예측/pilryoung_0111_v1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "292.3px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
